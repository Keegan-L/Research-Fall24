{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d240dc26-d186-4aca-85aa-06bc2b896727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f382e848160>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "from transformer_lens import HookedTransformer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a55ef437-c3f3-4bb8-9170-e32fc159e4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92668156267d402eb158816f22ab1adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c0cb02a3e244ef8350e8443220a017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36450b55ace24bc4b556ec5b0a360de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02fd622e49ed48b8a82a0bc8220bafa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6310919ae6e47829e338f54622f2ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b842a5d68e98456196c7bbed15665d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc939cd36564ab4a5f867e88e60e9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fa4406d-a0b1-4350-987f-0ed699052036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get top tokens from a prompt\n",
    "\n",
    "def get_top_predictions(prompt, top_k=10):\n",
    "    \"\"\"Get top k predicted tokens after a prompt.\"\"\"\n",
    "    input_tokens = model.to_tokens(prompt, prepend_bos=True)\n",
    "    logits = model(input_tokens)\n",
    "    probs = logits.softmax(dim=-1)\n",
    "    \n",
    "    # Get predictions for the last position\n",
    "    index = len(input_tokens[0])\n",
    "    token_probs = probs[:, index - 1]\n",
    "    sorted_token_probs, sorted_token_positions = token_probs.sort(descending=True)\n",
    "    \n",
    "    return [(model.to_string(sorted_token_positions[0, i]), \n",
    "             sorted_token_probs[0, i].item()) \n",
    "            for i in range(top_k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbbb7415-2088-48db-af36-2400a29533e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_unigrams(prompt=\"<|endoftext|>\", top_k=10):\n",
    "    #print(f\"Finding top {top_k} unigrams for prompt: {prompt}\")\n",
    "    top_unigrams = []\n",
    "    embeddings = {}\n",
    "    \n",
    "    predictions = get_top_predictions(prompt, top_k=top_k)  # Pass parameters to get_top_predictions\n",
    "    for token, prob in predictions:\n",
    "        print(f\"Token: |{token}| Probability: {prob:.2%}\")\n",
    "        top_unigrams.append(token)\n",
    "        \n",
    "        # Get embedding for this unigram\n",
    "        tokens = model.to_tokens(token, prepend_bos=True)\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        embedding = cache[\"blocks.8.hook_resid_post\"][:, -1, :]\n",
    "        embeddings[token] = embedding\n",
    "\n",
    "    return top_unigrams, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1bf62fd-2de7-4dae-8426-1e6edd16b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get embeddings\n",
    "def get_embedding(word, layer):\n",
    "    tokens = model.to_tokens(word, prepend_bos=True)\n",
    "    _, cache = model.run_with_cache(tokens)\n",
    "    \n",
    "    embedding = cache[\"blocks.\" + str(layer) +\".hook_resid_post\"][:, -1, :]\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb948a3f-71b6-4c07-abcd-16a22f80b45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting top 10 predictions for prompt: '<|endoftext|>'\n",
      "Token: |The| Probability: 7.68%\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'blocks.8.hook_resid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m unigrams, unigram_embeddings, similarities\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Usage example:\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Get initial unigrams\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m unigrams, embeddings, similarities \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<|endoftext|>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Visualize results\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_similarities\u001b[39m(unigrams, similarities):\n",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(prompt, layer, top_k)\u001b[0m\n\u001b[1;32m     23\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto_tokens(token, prepend_bos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m     _, cache \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mrun_with_cache(tokens)\n\u001b[0;32m---> 25\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlayer\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.hook_resid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     26\u001b[0m     unigram_embeddings[token] \u001b[38;5;241m=\u001b[39m embedding\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 3. Calculate similarities\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformer_lens/ActivationCache.py:168\u001b[0m, in \u001b[0;36mActivationCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_dict[key]\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(key) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_act_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'blocks.8.hook_resid'"
     ]
    }
   ],
   "source": [
    "def main(prompt=\"<|endoftext|>\", layer=8, top_k=10):\n",
    "    \"\"\"\n",
    "    Main function to analyze predictions and embeddings.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): Text to get predictions after (default \"<|endoftext|>\" for unigrams)\n",
    "        layer (int): Which GPT-2 layer to get embeddings from (default 8)\n",
    "        top_k (int): Number of top predictions to get (default 10)\n",
    "    \"\"\"\n",
    "    # 1. Get top predictions\n",
    "    print(f\"\\nGetting top {top_k} predictions for prompt: '{prompt}'\")\n",
    "    top_predictions = get_top_predictions(prompt, top_k)\n",
    "    \n",
    "    # 2. Get embeddings for these predictions\n",
    "    unigram_embeddings = {}\n",
    "    unigrams = []\n",
    "    \n",
    "    for token, prob in top_predictions:\n",
    "        print(f\"Token: |{token}| Probability: {prob:.2%}\")\n",
    "        unigrams.append(token)\n",
    "        \n",
    "        # Get embedding from specified layer\n",
    "        tokens = model.to_tokens(token, prepend_bos=True)\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        embedding = cache[f\"blocks.{layer}.hook_resid_post\"][:, -1, :]\n",
    "        unigram_embeddings[token] = embedding\n",
    "    \n",
    "    # 3. Calculate similarities\n",
    "    similarities = {}\n",
    "    for i, word1 in enumerate(unigrams):\n",
    "        for j, word2 in enumerate(unigrams):\n",
    "            embed1 = unigram_embeddings[word1]\n",
    "            embed2 = unigram_embeddings[word2]\n",
    "            \n",
    "            similarities[(word1, word2)] = {\n",
    "                'cosine': F.cosine_similarity(embed1, embed2, dim=-1).item(),\n",
    "                'euclidean': torch.norm(embed1 - embed2, p=2).item(),\n",
    "                'manhattan': torch.norm(embed1 - embed2, p=1).item()\n",
    "            }\n",
    "    \n",
    "    return unigrams, unigram_embeddings, similarities\n",
    "\n",
    "# Usage example:\n",
    "# Get initial unigrams\n",
    "unigrams, embeddings, similarities = main(prompt=\"<|endoftext|>\", layer=8)\n",
    "\n",
    "# Visualize results\n",
    "def visualize_similarities(unigrams, similarities):\n",
    "    n = len(unigrams)\n",
    "    cosine_matrix = np.zeros((n, n))\n",
    "    euclidean_matrix = np.zeros((n, n))\n",
    "    manhattan_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i, word1 in enumerate(unigrams):\n",
    "        for j, word2 in enumerate(unigrams):\n",
    "            cosine_matrix[i,j] = similarities[(word1, word2)]['cosine']\n",
    "            euclidean_matrix[i,j] = similarities[(word1, word2)]['euclidean']\n",
    "            manhattan_matrix[i,j] = similarities[(word1, word2)]['manhattan']\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "    sns.heatmap(cosine_matrix, xticklabels=unigrams, yticklabels=unigrams,\n",
    "                cmap='viridis', annot=True, fmt='.2f', ax=ax1)\n",
    "    ax1.set_title('Cosine Similarity')\n",
    "\n",
    "    sns.heatmap(euclidean_matrix, xticklabels=unigrams, yticklabels=unigrams,\n",
    "                cmap='viridis', annot=True, fmt='.2f', ax=ax2)\n",
    "    ax2.set_title('Euclidean Distance')\n",
    "\n",
    "    sns.heatmap(manhattan_matrix, xticklabels=unigrams, yticklabels=unigrams,\n",
    "                cmap='viridis', annot=True, fmt='.2f', ax=ax3)\n",
    "    ax3.set_title('Manhattan Distance')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run and visualize\n",
    "unigrams, embeddings, similarities = main(prompt = \"my name is Keeg and I like my eggs a little runny\")\n",
    "visualize_similarities(unigrams, similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2c7638-d095-4042-bdce-f6da55993bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
