{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44938c24-b732-4235-8e02-ec08a2d620f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaTokenizer, LlamaForCausalLM\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import DictionaryLearning\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af814df-f926-40fc-9473-325d5dce6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_with_llm(prompt, model_name=\"decapoda-research/llama-7b-hf\", max_length=100):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "    model = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "    # Prepare input\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "        outputs_with_hidden = model(input_ids, output_hidden_states=True)\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Get the last hidden state\n",
    "    last_hidden_state = outputs_with_hidden.hidden_states[-1]\n",
    "\n",
    "    return generated_text, last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721979fe-aa7f-412c-985e-3c615fafb2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_dictionary_learning(data, n_components=15, alpha=1, max_iter=100, visualize=True):\n",
    "    \"\"\"\n",
    "    Perform dictionary learning on the input data.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: numpy array, shape (n_samples, n_features)\n",
    "    - n_components: int, number of dictionary elements to learn\n",
    "    - alpha: float, sparsity controlling parameter\n",
    "    - max_iter: int, maximum number of iterations\n",
    "    - visualize: bool, whether to visualize the results\n",
    "    \n",
    "    Returns:\n",
    "    - dictionary: learned dictionary\n",
    "    - sparse_code: sparse representation of data\n",
    "    \"\"\"\n",
    "    # Perform dictionary learning\n",
    "    dl = DictionaryLearning(n_components=n_components, alpha=alpha, max_iter=max_iter, random_state=0)\n",
    "    dictionary = dl.fit(data).components_\n",
    "    \n",
    "    # Transform data using the learned dictionary\n",
    "    sparse_code = dl.transform(data)\n",
    "    \n",
    "    if visualize:\n",
    "        visualize_results(data, dictionary, sparse_code)\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"Original data shape: {data.shape}\")\n",
    "    print(f\"Learned dictionary shape: {dictionary.shape}\")\n",
    "    print(f\"Sparse codes shape: {sparse_code.shape}\")\n",
    "    print(f\"Average sparsity: {np.mean(sparse_code != 0):.2%}\")\n",
    "    \n",
    "    return dictionary, sparse_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104262e9-afce-46ba-b33b-34f2e0c4af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(data, dictionary, sparse_code):\n",
    "    \"\"\"\n",
    "    Visualize the original data, learned dictionary, and sparse codes.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original data\n",
    "    ax1.imshow(data.T, aspect='auto', interpolation='nearest')\n",
    "    ax1.set_title(\"Original Data\")\n",
    "    ax1.set_xlabel(\"Samples\")\n",
    "    ax1.set_ylabel(\"Features\")\n",
    "    \n",
    "    # Learned dictionary\n",
    "    ax2.imshow(dictionary, aspect='auto', interpolation='nearest')\n",
    "    ax2.set_title(\"Learned Dictionary\")\n",
    "    ax2.set_xlabel(\"Components\")\n",
    "    ax2.set_ylabel(\"Features\")\n",
    "    \n",
    "    # Sparse codes\n",
    "    ax3.imshow(sparse_code.T, aspect='auto', interpolation='nearest')\n",
    "    ax3.set_title(\"Sparse Codes\")\n",
    "    ax3.set_xlabel(\"Samples\")\n",
    "    ax3.set_ylabel(\"Components\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44fb7d1-8d4f-4103-affc-c3a6725e5dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
