{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e8fe78-b64f-44e1-9be3-abbf469f677c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sae_lens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformer_lens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HookedTransformer\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msae_lens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SAE\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Dict\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sae_lens'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "import numpy as np\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c8ad73-31d8-4740-b870-bf81d26e3829",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAEAnalyzer:\n",
    "    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize base model and SAE\n",
    "        print(\"Loading models...\")\n",
    "        self.model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "        \n",
    "        # Load the SAE\n",
    "        self.sae, self.cfg_dict, self.sparsity = SAE.from_pretrained(\n",
    "            release=\"gpt2-small-res-jb\",\n",
    "            sae_id=\"blocks.8.hook_resid_pre\",\n",
    "            device=device,\n",
    "        )\n",
    "        \n",
    "    def get_first_n_sentences(self, text: str, n: int = 5) -> List[str]:\n",
    "        \"\"\"Extract first n sentences from text.\"\"\"\n",
    "        sentences = []\n",
    "        current = []\n",
    "        \n",
    "        for char in text:\n",
    "            current.append(char)\n",
    "            if char in '.!?':\n",
    "                sentence = ''.join(current).strip()\n",
    "                if sentence and not sentence.endswith(('Mr.', 'Mrs.', 'Dr.', 'Ms.')):\n",
    "                    sentences.append(sentence)\n",
    "                    if len(sentences) >= n:\n",
    "                        break\n",
    "                    current = []\n",
    "                    \n",
    "        return sentences[:n]\n",
    "    \n",
    "    def get_first_n_paragraphs(self, text: str, n: int = 5) -> List[str]:\n",
    "        \"\"\"Extract first n paragraphs from text.\"\"\"\n",
    "        paragraphs = []\n",
    "        current = []\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                current.append(line.strip())\n",
    "            elif current:\n",
    "                paragraphs.append(' '.join(current))\n",
    "                current = []\n",
    "                if len(paragraphs) >= n:\n",
    "                    break\n",
    "                    \n",
    "        if current and len(paragraphs) < n:\n",
    "            paragraphs.append(' '.join(current))\n",
    "            \n",
    "        return paragraphs[:n]\n",
    "    \n",
    "    def get_active_neurons(self, text: str) -> Dict:\n",
    "        \"\"\"Get active neurons for a piece of text.\"\"\"\n",
    "        tokens = self.model.to_tokens(text, prepend_bos=True)\n",
    "        \n",
    "        # Run the model and get activations at the SAE's hook point\n",
    "        _, cache = self.model.run_with_cache(\n",
    "            tokens,\n",
    "            names_filter=lambda name: name == self.sae.cfg.hook_name\n",
    "        )\n",
    "        \n",
    "        # Get activations at the hook point\n",
    "        activations = cache[self.sae.cfg.hook_name]\n",
    "        \n",
    "        # Encode activations with SAE\n",
    "        encoded = self.sae.encode(activations)\n",
    "        \n",
    "        # Find most active neurons\n",
    "        # Shape: [batch, pos, features]\n",
    "        neuron_activities = encoded.abs().mean(dim=1)  # Average over positions\n",
    "        top_neurons = torch.topk(neuron_activities, k=10, dim=-1)\n",
    "        \n",
    "        return {\n",
    "            'indices': top_neurons.indices.cpu().numpy(),\n",
    "            'values': top_neurons.values.cpu().numpy(),\n",
    "            'encoded': encoded.cpu().numpy()\n",
    "        }\n",
    "    \n",
    "    def analyze_text_file(self, file_path: str) -> Dict:\n",
    "        \"\"\"Analyze a text file at different levels.\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            full_text = file.read()\n",
    "            \n",
    "        # Get text at different levels\n",
    "        sentences = self.get_first_n_sentences(full_text, 5)\n",
    "        paragraphs = self.get_first_n_paragraphs(full_text, 5)\n",
    "        \n",
    "        results = {\n",
    "            'sentences': {\n",
    "                'texts': sentences,\n",
    "                'neurons': [self.get_active_neurons(s) for s in sentences]\n",
    "            },\n",
    "            'paragraphs': {\n",
    "                'texts': paragraphs,\n",
    "                'neurons': [self.get_active_neurons(p) for p in paragraphs]\n",
    "            },\n",
    "            'full_text': {\n",
    "                'text': full_text,\n",
    "                'neurons': self.get_active_neurons(full_text)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca8655-2623-414e-98b4-7b0083bb5ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_all_files(data_dir: str) -> Dict:\n",
    "    \"\"\"Analyze all text files in directory.\"\"\"\n",
    "    analyzer = SAEAnalyzer()\n",
    "    all_results = {}\n",
    "    \n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            print(f\"\\nProcessing {filename}...\")\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            category = filename.replace('.txt', '')\n",
    "            all_results[category] = analyzer.analyze_text_file(file_path)\n",
    "            \n",
    "            # Print summary for this category\n",
    "            print(f\"\\nResults for {category}:\")\n",
    "            \n",
    "            # Show top neurons for each level\n",
    "            for level in ['sentences', 'paragraphs', 'full_text']:\n",
    "                print(f\"\\n{level.upper()}:\")\n",
    "                if level in ['sentences', 'paragraphs']:\n",
    "                    for i, result in enumerate(all_results[category][level]['neurons']):\n",
    "                        print(f\"\\n{level[:-1].capitalize()} {i+1}:\")\n",
    "                        print(f\"Top 10 active neurons: {result['indices']}\")\n",
    "                        print(f\"Activation values: {result['values'].round(3)}\")\n",
    "                else:\n",
    "                    result = all_results[category][level]['neurons']\n",
    "                    print(\"\\nTop 10 active neurons:\", result['indices'])\n",
    "                    print(\"Activation values:\", result['values'].round(3))\n",
    "                    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0288e688-5067-4362-a087-f1e68484b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_dir = \"./data\"\n",
    "    results = analyze_all_files(data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
