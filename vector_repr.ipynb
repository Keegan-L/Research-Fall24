{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac1558a-1ba2-4855-ab30-5281c4e16ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f9d0210-6ac3-4469-8224-7d5185e50672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self, model_name=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    def get_first_n_sentences(self, text: str, n: int = 5) -> List[str]:\n",
    "        \"\"\"Get only the first n sentences from text.\"\"\"\n",
    "        sentences = []\n",
    "        current_sentence = []\n",
    "        \n",
    "        for char in text:\n",
    "            current_sentence.append(char)\n",
    "            if char in '.!?':\n",
    "                # Check for actual end of sentence (not e.g. \"Mr.\")\n",
    "                sentence = ''.join(current_sentence).strip()\n",
    "                if sentence and not sentence.endswith(('Mr.', 'Mrs.', 'Dr.', 'Ms.')):\n",
    "                    sentences.append(sentence)\n",
    "                    if len(sentences) >= n:\n",
    "                        break\n",
    "                    current_sentence = []\n",
    "            \n",
    "        return sentences\n",
    "    \n",
    "    def get_first_n_paragraphs(self, text: str, n: int = 5) -> List[str]:\n",
    "        \"\"\"Get only the first n paragraphs from text.\"\"\"\n",
    "        paragraphs = []\n",
    "        current = []\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                current.append(line.strip())\n",
    "            elif current:  # Empty line and we have content\n",
    "                paragraphs.append(' '.join(current))\n",
    "                current = []\n",
    "                if len(paragraphs) >= n:\n",
    "                    break\n",
    "        \n",
    "        # Add the last paragraph if we haven't reached n\n",
    "        if current and len(paragraphs) < n:\n",
    "            paragraphs.append(' '.join(current))\n",
    "            \n",
    "        return paragraphs[:n]\n",
    "    \n",
    "    def get_embedding(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"Get embedding for a piece of text using TinyLlama.\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embedding\n",
    "    \n",
    "    def process_text_file(self, file_path: str) -> Dict[str, Dict]:\n",
    "        \"\"\"Process a text file and return first n sentences, paragraphs, and full text.\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            full_text = file.read()\n",
    "        \n",
    "        # Get exactly 5 sentences and 5 paragraphs\n",
    "        first_sentences = self.get_first_n_sentences(full_text, 5)\n",
    "        first_paragraphs = self.get_first_n_paragraphs(full_text, 5)\n",
    "        \n",
    "        result = {\n",
    "            'sentences': {\n",
    "                'texts': first_sentences,\n",
    "                'embeddings': [self.get_embedding(s) for s in first_sentences]\n",
    "            },\n",
    "            'paragraphs': {\n",
    "                'texts': first_paragraphs,\n",
    "                'embeddings': [self.get_embedding(p) for p in first_paragraphs]\n",
    "            },\n",
    "            'full_text': {\n",
    "                'text': full_text,\n",
    "                'embedding': self.get_embedding(full_text)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9b1aa19-f25d-4b0e-afbf-71cd7820496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_files(data_dir: str) -> Dict[str, Dict]:\n",
    "    \"\"\"Process all text files in the data directory.\"\"\"\n",
    "    processor = TextProcessor()\n",
    "    all_results = {}\n",
    "    \n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            category = filename.replace('.txt', '')\n",
    "            print(f\"Processing {category}...\")\n",
    "            all_results[category] = processor.process_text_file(file_path)\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a37afb5b-265a-4d50-b7c3-b6022dd06546",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_all_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Print summary for each category\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m category, data \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mprocess_all_files\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m      3\u001b[0m processor \u001b[38;5;241m=\u001b[39m TextProcessor()\n\u001b[1;32m      4\u001b[0m all_results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      8\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_dir = \"./data\"\n",
    "    results = process_all_files(data_dir)\n",
    "    \n",
    "    # Print summary for each category\n",
    "    for category, data in results.items():\n",
    "        print(f\"\\nCategory: {category}\")\n",
    "        print(f\"Number of sentences processed: {len(data['sentences']['texts'])}\")\n",
    "        print(f\"Number of paragraphs processed: {len(data['paragraphs']['texts'])}\")\n",
    "        print(\"\\nFirst sentence:\", data['sentences']['texts'][0][:100], \"...\")\n",
    "        print(\"\\nFirst paragraph:\", data['paragraphs']['texts'][0][:100], \"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
